{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Bidirectional, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./storage/writer/train.csv', encoding = 'utf-8') \n",
    "test = pd.read_csv('./storage/writer/test_x.csv', encoding = 'utf-8') \n",
    "ss = pd.read_csv('./storage/writer/sample_submission.csv', encoding = 'utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 부호 제거하는 함수 \n",
    "def alpha_num(text): \n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', text) \n",
    "\n",
    "train['text'] = train['text'].apply(alpha_num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거하는 함수 \n",
    "def remove_stopwords(text): \n",
    "    final_text = [] \n",
    "    for i in text.split(): \n",
    "        if i.strip().lower() not in stopwords: \n",
    "            final_text.append(i.strip()) \n",
    "    return \" \".join(final_text)\n",
    "\n",
    "# 불용어\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
    "             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n",
    "             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n",
    "             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n",
    "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
    "             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n",
    "             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
    "             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n",
    "             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n",
    "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "train['text'] = train['text'].str.lower() \n",
    "test['text'] = test['text'].str.lower() \n",
    "train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords) \n",
    "test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([x for x in train['text']]) \n",
    "X_test = np.array([x for x in test['text']])            \n",
    "y_train = np.array([x for x in train['author']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54879,), (54879,), (19617,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embedding_dim = 16\n",
    "max_length = 500 \n",
    "padding_type='post' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size) \n",
    "tokenizer.fit_on_texts(X_train) \n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(X_train) \n",
    "train_padded = pad_sequences(train_sequences, padding = padding_type, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(X_test) \n",
    "test_padded = pad_sequences(test_sequences, padding = padding_type, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54879, 500), (19617, 500))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded.shape, test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_gru(): \n",
    "    model = Sequential() \n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length = max_length)) \n",
    "    model.add(Bidirectional(GRU(150, return_sequences = True))) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(100)) \n",
    "    model.add(Dense(32, activation = 'relu', kernel_regularizer = regularizers.l2(0.01)))\n",
    "    model.add(Dense(5, activation = 'softmax')) \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lstm(): \n",
    "    model = Sequential() \n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length = max_length)) \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(128, 5, padding = 'valid', activation = 'relu', strides = 3)) \n",
    "    model.add(Conv1D(128, 5, padding = 'valid', activation = 'relu', strides = 3)) \n",
    "    model.add(MaxPooling1D(pool_size = 4))  \n",
    "    model.add(LSTM(55)) \n",
    "    model.add(Dense(5, activation = 'softmax')) \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Validating on Fold 1 ...\n",
      "Train on 43903 samples, validate on 10976 samples\n",
      "Epoch 1/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 1.4128 - accuracy: 0.3751\n",
      "Epoch 00001: val_loss improved from inf to 1.21595, saving model to ./storage/writer_trainfiles/kfold1/epoch_001_val_1.216.h5\n",
      "43903/43903 [==============================] - 22s 497us/sample - loss: 1.4126 - accuracy: 0.3751 - val_loss: 1.2159 - val_accuracy: 0.4918\n",
      "Epoch 2/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 1.0798 - accuracy: 0.5605\n",
      "Epoch 00002: val_loss improved from 1.21595 to 0.95228, saving model to ./storage/writer_trainfiles/kfold1/epoch_002_val_0.952.h5\n",
      "43903/43903 [==============================] - 19s 431us/sample - loss: 1.0799 - accuracy: 0.5604 - val_loss: 0.9523 - val_accuracy: 0.6221\n",
      "Epoch 3/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.8776 - accuracy: 0.6591\n",
      "Epoch 00003: val_loss improved from 0.95228 to 0.87902, saving model to ./storage/writer_trainfiles/kfold1/epoch_003_val_0.879.h5\n",
      "43903/43903 [==============================] - 19s 429us/sample - loss: 0.8774 - accuracy: 0.6592 - val_loss: 0.8790 - val_accuracy: 0.6574\n",
      "Epoch 4/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.7761 - accuracy: 0.7023\n",
      "Epoch 00004: val_loss improved from 0.87902 to 0.83140, saving model to ./storage/writer_trainfiles/kfold1/epoch_004_val_0.831.h5\n",
      "43903/43903 [==============================] - 19s 429us/sample - loss: 0.7762 - accuracy: 0.7023 - val_loss: 0.8314 - val_accuracy: 0.6805\n",
      "Epoch 5/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.7020 - accuracy: 0.7372\n",
      "Epoch 00005: val_loss improved from 0.83140 to 0.80716, saving model to ./storage/writer_trainfiles/kfold1/epoch_005_val_0.807.h5\n",
      "43903/43903 [==============================] - 19s 430us/sample - loss: 0.7020 - accuracy: 0.7372 - val_loss: 0.8072 - val_accuracy: 0.6979\n",
      "Epoch 6/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.6455 - accuracy: 0.7600\n",
      "Epoch 00006: val_loss improved from 0.80716 to 0.79786, saving model to ./storage/writer_trainfiles/kfold1/epoch_006_val_0.798.h5\n",
      "43903/43903 [==============================] - 19s 430us/sample - loss: 0.6456 - accuracy: 0.7599 - val_loss: 0.7979 - val_accuracy: 0.6990\n",
      "Epoch 7/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.6114 - accuracy: 0.7714\n",
      "Epoch 00007: val_loss improved from 0.79786 to 0.77939, saving model to ./storage/writer_trainfiles/kfold1/epoch_007_val_0.779.h5\n",
      "43903/43903 [==============================] - 19s 433us/sample - loss: 0.6114 - accuracy: 0.7714 - val_loss: 0.7794 - val_accuracy: 0.7096\n",
      "Epoch 8/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.5798 - accuracy: 0.7859\n",
      "Epoch 00008: val_loss did not improve from 0.77939\n",
      "43903/43903 [==============================] - 19s 422us/sample - loss: 0.5798 - accuracy: 0.7859 - val_loss: 0.8142 - val_accuracy: 0.7026\n",
      "Epoch 9/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.5490 - accuracy: 0.7946\n",
      "Epoch 00009: val_loss did not improve from 0.77939\n",
      "43903/43903 [==============================] - 18s 419us/sample - loss: 0.5490 - accuracy: 0.7946 - val_loss: 0.7868 - val_accuracy: 0.7135\n",
      "Epoch 10/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.5282 - accuracy: 0.8038\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.77939\n",
      "43903/43903 [==============================] - 18s 417us/sample - loss: 0.5285 - accuracy: 0.8036 - val_loss: 0.7880 - val_accuracy: 0.7143\n",
      "Epoch 11/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.4941 - accuracy: 0.8160\n",
      "Epoch 00011: val_loss improved from 0.77939 to 0.77066, saving model to ./storage/writer_trainfiles/kfold1/epoch_011_val_0.771.h5\n",
      "43903/43903 [==============================] - 19s 426us/sample - loss: 0.4944 - accuracy: 0.8157 - val_loss: 0.7707 - val_accuracy: 0.7192\n",
      "Epoch 12/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.4756 - accuracy: 0.8243\n",
      "Epoch 00012: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 18s 419us/sample - loss: 0.4757 - accuracy: 0.8242 - val_loss: 0.7976 - val_accuracy: 0.7177\n",
      "Epoch 13/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.4656 - accuracy: 0.8261\n",
      "Epoch 00013: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 18s 417us/sample - loss: 0.4656 - accuracy: 0.8261 - val_loss: 0.8243 - val_accuracy: 0.7136\n",
      "Epoch 14/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.4498 - accuracy: 0.8313\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 19s 422us/sample - loss: 0.4497 - accuracy: 0.8312 - val_loss: 0.8114 - val_accuracy: 0.7209\n",
      "Epoch 15/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.4245 - accuracy: 0.8427\n",
      "Epoch 00015: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 18s 418us/sample - loss: 0.4246 - accuracy: 0.8426 - val_loss: 0.8165 - val_accuracy: 0.7219\n",
      "Epoch 16/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.4179 - accuracy: 0.8450\n",
      "Epoch 00016: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 18s 420us/sample - loss: 0.4180 - accuracy: 0.8450 - val_loss: 0.8367 - val_accuracy: 0.7173\n",
      "Epoch 17/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.4080 - accuracy: 0.8494\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 19s 424us/sample - loss: 0.4080 - accuracy: 0.8494 - val_loss: 0.8538 - val_accuracy: 0.7228\n",
      "Epoch 18/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.3936 - accuracy: 0.8536\n",
      "Epoch 00018: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 19s 423us/sample - loss: 0.3935 - accuracy: 0.8536 - val_loss: 0.8396 - val_accuracy: 0.7250\n",
      "Epoch 19/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.3840 - accuracy: 0.8561\n",
      "Epoch 00019: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 18s 419us/sample - loss: 0.3843 - accuracy: 0.8560 - val_loss: 0.8439 - val_accuracy: 0.7198\n",
      "Epoch 20/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8598\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 18s 421us/sample - loss: 0.3759 - accuracy: 0.8598 - val_loss: 0.8640 - val_accuracy: 0.7205\n",
      "Epoch 21/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8626\n",
      "Epoch 00021: val_loss did not improve from 0.77066\n",
      "43903/43903 [==============================] - 18s 420us/sample - loss: 0.3646 - accuracy: 0.8628 - val_loss: 0.8621 - val_accuracy: 0.7228\n",
      "... Validating on Fold 2 ...\n",
      "Train on 43903 samples, validate on 10976 samples\n",
      "Epoch 1/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 1.4240 - accuracy: 0.3477\n",
      "Epoch 00001: val_loss improved from inf to 1.27951, saving model to ./storage/writer_trainfiles/kfold2/epoch_001_val_1.280.h5\n",
      "43903/43903 [==============================] - 20s 464us/sample - loss: 1.4238 - accuracy: 0.3478 - val_loss: 1.2795 - val_accuracy: 0.4254\n",
      "Epoch 2/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 1.2112 - accuracy: 0.4645\n",
      "Epoch 00002: val_loss improved from 1.27951 to 1.07872, saving model to ./storage/writer_trainfiles/kfold2/epoch_002_val_1.079.h5\n",
      "43903/43903 [==============================] - 18s 415us/sample - loss: 1.2112 - accuracy: 0.4646 - val_loss: 1.0787 - val_accuracy: 0.5506\n",
      "Epoch 3/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.9800 - accuracy: 0.6042\n",
      "Epoch 00003: val_loss improved from 1.07872 to 0.91536, saving model to ./storage/writer_trainfiles/kfold2/epoch_003_val_0.915.h5\n",
      "43903/43903 [==============================] - 18s 417us/sample - loss: 0.9800 - accuracy: 0.6043 - val_loss: 0.9154 - val_accuracy: 0.6447\n",
      "Epoch 4/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.8098 - accuracy: 0.6904\n",
      "Epoch 00004: val_loss improved from 0.91536 to 0.85316, saving model to ./storage/writer_trainfiles/kfold2/epoch_004_val_0.853.h5\n",
      "43903/43903 [==============================] - 18s 415us/sample - loss: 0.8100 - accuracy: 0.6902 - val_loss: 0.8532 - val_accuracy: 0.6731\n",
      "Epoch 5/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.7152 - accuracy: 0.7293\n",
      "Epoch 00005: val_loss improved from 0.85316 to 0.80167, saving model to ./storage/writer_trainfiles/kfold2/epoch_005_val_0.802.h5\n",
      "43903/43903 [==============================] - 18s 417us/sample - loss: 0.7151 - accuracy: 0.7292 - val_loss: 0.8017 - val_accuracy: 0.6994\n",
      "Epoch 6/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.6585 - accuracy: 0.7532\n",
      "Epoch 00006: val_loss improved from 0.80167 to 0.79012, saving model to ./storage/writer_trainfiles/kfold2/epoch_006_val_0.790.h5\n",
      "43903/43903 [==============================] - 18s 416us/sample - loss: 0.6585 - accuracy: 0.7532 - val_loss: 0.7901 - val_accuracy: 0.7047\n",
      "Epoch 7/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.6099 - accuracy: 0.7714\n",
      "Epoch 00007: val_loss improved from 0.79012 to 0.78400, saving model to ./storage/writer_trainfiles/kfold2/epoch_007_val_0.784.h5\n",
      "43903/43903 [==============================] - 18s 418us/sample - loss: 0.6101 - accuracy: 0.7714 - val_loss: 0.7840 - val_accuracy: 0.7145\n",
      "Epoch 8/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.5791 - accuracy: 0.7836\n",
      "Epoch 00008: val_loss improved from 0.78400 to 0.77809, saving model to ./storage/writer_trainfiles/kfold2/epoch_008_val_0.778.h5\n",
      "43903/43903 [==============================] - 18s 413us/sample - loss: 0.5790 - accuracy: 0.7835 - val_loss: 0.7781 - val_accuracy: 0.7147\n",
      "Epoch 9/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.5565 - accuracy: 0.7925\n",
      "Epoch 00009: val_loss improved from 0.77809 to 0.77764, saving model to ./storage/writer_trainfiles/kfold2/epoch_009_val_0.778.h5\n",
      "43903/43903 [==============================] - 19s 425us/sample - loss: 0.5565 - accuracy: 0.7925 - val_loss: 0.7776 - val_accuracy: 0.7136\n",
      "Epoch 10/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.5281 - accuracy: 0.8026\n",
      "Epoch 00010: val_loss improved from 0.77764 to 0.77569, saving model to ./storage/writer_trainfiles/kfold2/epoch_010_val_0.776.h5\n",
      "43903/43903 [==============================] - 18s 415us/sample - loss: 0.5284 - accuracy: 0.8025 - val_loss: 0.7757 - val_accuracy: 0.7130\n",
      "Epoch 11/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.5069 - accuracy: 0.8105\n",
      "Epoch 00011: val_loss did not improve from 0.77569\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.5071 - accuracy: 0.8104 - val_loss: 0.7794 - val_accuracy: 0.7216\n",
      "Epoch 12/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.4899 - accuracy: 0.8186\n",
      "Epoch 00012: val_loss did not improve from 0.77569\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.4897 - accuracy: 0.8186 - val_loss: 0.7946 - val_accuracy: 0.7211\n",
      "Epoch 13/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.4783 - accuracy: 0.8216\n",
      "Epoch 00013: val_loss improved from 0.77569 to 0.76717, saving model to ./storage/writer_trainfiles/kfold2/epoch_013_val_0.767.h5\n",
      "43903/43903 [==============================] - 18s 419us/sample - loss: 0.4784 - accuracy: 0.8216 - val_loss: 0.7672 - val_accuracy: 0.7228\n",
      "Epoch 14/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.4633 - accuracy: 0.8287\n",
      "Epoch 00014: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.4633 - accuracy: 0.8287 - val_loss: 0.7945 - val_accuracy: 0.7233\n",
      "Epoch 15/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.4478 - accuracy: 0.8332\n",
      "Epoch 00015: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.4475 - accuracy: 0.8332 - val_loss: 0.8025 - val_accuracy: 0.7255\n",
      "Epoch 16/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.4372 - accuracy: 0.8374\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.4374 - accuracy: 0.8374 - val_loss: 0.8080 - val_accuracy: 0.7222\n",
      "Epoch 17/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.4157 - accuracy: 0.8452\n",
      "Epoch 00017: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 407us/sample - loss: 0.4155 - accuracy: 0.8452 - val_loss: 0.8112 - val_accuracy: 0.7229\n",
      "Epoch 18/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3982 - accuracy: 0.8532\n",
      "Epoch 00018: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 404us/sample - loss: 0.3983 - accuracy: 0.8532 - val_loss: 0.7894 - val_accuracy: 0.7271\n",
      "Epoch 19/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.3935 - accuracy: 0.8540\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.3935 - accuracy: 0.8540 - val_loss: 0.7948 - val_accuracy: 0.7223\n",
      "Epoch 20/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.8594\n",
      "Epoch 00020: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 404us/sample - loss: 0.3787 - accuracy: 0.8595 - val_loss: 0.8334 - val_accuracy: 0.7280\n",
      "Epoch 21/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.3651 - accuracy: 0.8637\n",
      "Epoch 00021: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 401us/sample - loss: 0.3650 - accuracy: 0.8638 - val_loss: 0.8305 - val_accuracy: 0.7245\n",
      "Epoch 22/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3596 - accuracy: 0.8656\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 404us/sample - loss: 0.3595 - accuracy: 0.8657 - val_loss: 0.8383 - val_accuracy: 0.7247\n",
      "Epoch 23/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.8704\n",
      "Epoch 00023: val_loss did not improve from 0.76717\n",
      "43903/43903 [==============================] - 18s 408us/sample - loss: 0.3482 - accuracy: 0.8704 - val_loss: 0.8857 - val_accuracy: 0.7203\n",
      "... Validating on Fold 3 ...\n",
      "Train on 43903 samples, validate on 10976 samples\n",
      "Epoch 1/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 1.4027 - accuracy: 0.3738\n",
      "Epoch 00001: val_loss improved from inf to 1.22774, saving model to ./storage/writer_trainfiles/kfold3/epoch_001_val_1.228.h5\n",
      "43903/43903 [==============================] - 20s 463us/sample - loss: 1.4027 - accuracy: 0.3739 - val_loss: 1.2277 - val_accuracy: 0.4848\n",
      "Epoch 2/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 1.0991 - accuracy: 0.5485\n",
      "Epoch 00002: val_loss improved from 1.22774 to 0.96772, saving model to ./storage/writer_trainfiles/kfold3/epoch_002_val_0.968.h5\n",
      "43903/43903 [==============================] - 18s 418us/sample - loss: 1.0990 - accuracy: 0.5484 - val_loss: 0.9677 - val_accuracy: 0.6195\n",
      "Epoch 3/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.8977 - accuracy: 0.6487\n",
      "Epoch 00003: val_loss improved from 0.96772 to 0.90523, saving model to ./storage/writer_trainfiles/kfold3/epoch_003_val_0.905.h5\n",
      "43903/43903 [==============================] - 18s 418us/sample - loss: 0.8977 - accuracy: 0.6487 - val_loss: 0.9052 - val_accuracy: 0.6483\n",
      "Epoch 4/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.7864 - accuracy: 0.6972\n",
      "Epoch 00004: val_loss improved from 0.90523 to 0.82195, saving model to ./storage/writer_trainfiles/kfold3/epoch_004_val_0.822.h5\n",
      "43903/43903 [==============================] - 18s 420us/sample - loss: 0.7861 - accuracy: 0.6973 - val_loss: 0.8220 - val_accuracy: 0.6879\n",
      "Epoch 5/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.7071 - accuracy: 0.7318\n",
      "Epoch 00005: val_loss improved from 0.82195 to 0.80481, saving model to ./storage/writer_trainfiles/kfold3/epoch_005_val_0.805.h5\n",
      "43903/43903 [==============================] - 18s 416us/sample - loss: 0.7072 - accuracy: 0.7316 - val_loss: 0.8048 - val_accuracy: 0.6963\n",
      "Epoch 6/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.6540 - accuracy: 0.7547\n",
      "Epoch 00006: val_loss improved from 0.80481 to 0.79973, saving model to ./storage/writer_trainfiles/kfold3/epoch_006_val_0.800.h5\n",
      "43903/43903 [==============================] - 18s 416us/sample - loss: 0.6545 - accuracy: 0.7545 - val_loss: 0.7997 - val_accuracy: 0.7019\n",
      "Epoch 7/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.6126 - accuracy: 0.7720\n",
      "Epoch 00007: val_loss improved from 0.79973 to 0.76926, saving model to ./storage/writer_trainfiles/kfold3/epoch_007_val_0.769.h5\n",
      "43903/43903 [==============================] - 19s 421us/sample - loss: 0.6126 - accuracy: 0.7721 - val_loss: 0.7693 - val_accuracy: 0.7131\n",
      "Epoch 8/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.5828 - accuracy: 0.7825\n",
      "Epoch 00008: val_loss did not improve from 0.76926\n",
      "43903/43903 [==============================] - 18s 405us/sample - loss: 0.5828 - accuracy: 0.7825 - val_loss: 0.7902 - val_accuracy: 0.7025\n",
      "Epoch 9/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.5576 - accuracy: 0.7916\n",
      "Epoch 00009: val_loss did not improve from 0.76926\n",
      "43903/43903 [==============================] - 18s 411us/sample - loss: 0.5577 - accuracy: 0.7916 - val_loss: 0.8030 - val_accuracy: 0.7123\n",
      "Epoch 10/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.5339 - accuracy: 0.8020\n",
      "Epoch 00010: val_loss improved from 0.76926 to 0.76843, saving model to ./storage/writer_trainfiles/kfold3/epoch_010_val_0.768.h5\n",
      "43903/43903 [==============================] - 18s 419us/sample - loss: 0.5339 - accuracy: 0.8020 - val_loss: 0.7684 - val_accuracy: 0.7192\n",
      "Epoch 11/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.5151 - accuracy: 0.8097\n",
      "Epoch 00011: val_loss did not improve from 0.76843\n",
      "43903/43903 [==============================] - 18s 408us/sample - loss: 0.5149 - accuracy: 0.8098 - val_loss: 0.7707 - val_accuracy: 0.7239\n",
      "Epoch 12/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.8147\n",
      "Epoch 00012: val_loss did not improve from 0.76843\n",
      "43903/43903 [==============================] - 18s 408us/sample - loss: 0.4982 - accuracy: 0.8147 - val_loss: 0.7849 - val_accuracy: 0.7182\n",
      "Epoch 13/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.4790 - accuracy: 0.8212\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.76843\n",
      "43903/43903 [==============================] - 18s 407us/sample - loss: 0.4794 - accuracy: 0.8211 - val_loss: 0.7885 - val_accuracy: 0.7223\n",
      "Epoch 14/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.4534 - accuracy: 0.8327\n",
      "Epoch 00014: val_loss did not improve from 0.76843\n",
      "43903/43903 [==============================] - 18s 408us/sample - loss: 0.4536 - accuracy: 0.8326 - val_loss: 0.7791 - val_accuracy: 0.7280\n",
      "Epoch 15/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8396\n",
      "Epoch 00015: val_loss improved from 0.76843 to 0.75971, saving model to ./storage/writer_trainfiles/kfold3/epoch_015_val_0.760.h5\n",
      "43903/43903 [==============================] - 18s 419us/sample - loss: 0.4358 - accuracy: 0.8395 - val_loss: 0.7597 - val_accuracy: 0.7309\n",
      "Epoch 16/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.4240 - accuracy: 0.8434\n",
      "Epoch 00016: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.4238 - accuracy: 0.8436 - val_loss: 0.7912 - val_accuracy: 0.7266\n",
      "Epoch 17/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.4128 - accuracy: 0.8466\n",
      "Epoch 00017: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.4128 - accuracy: 0.8466 - val_loss: 0.8039 - val_accuracy: 0.7277\n",
      "Epoch 18/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.4061 - accuracy: 0.8488\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 411us/sample - loss: 0.4066 - accuracy: 0.8487 - val_loss: 0.8055 - val_accuracy: 0.7255\n",
      "Epoch 19/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3875 - accuracy: 0.8548\n",
      "Epoch 00019: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 405us/sample - loss: 0.3875 - accuracy: 0.8547 - val_loss: 0.8267 - val_accuracy: 0.7273\n",
      "Epoch 20/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.8585\n",
      "Epoch 00020: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 407us/sample - loss: 0.3800 - accuracy: 0.8586 - val_loss: 0.8168 - val_accuracy: 0.7322\n",
      "Epoch 21/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8590\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 409us/sample - loss: 0.3751 - accuracy: 0.8591 - val_loss: 0.8132 - val_accuracy: 0.7286\n",
      "Epoch 22/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.8662\n",
      "Epoch 00022: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 410us/sample - loss: 0.3596 - accuracy: 0.8663 - val_loss: 0.8319 - val_accuracy: 0.7287\n",
      "Epoch 23/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.8674\n",
      "Epoch 00023: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 414us/sample - loss: 0.3540 - accuracy: 0.8674 - val_loss: 0.8353 - val_accuracy: 0.7310\n",
      "Epoch 24/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8695\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 408us/sample - loss: 0.3484 - accuracy: 0.8696 - val_loss: 0.8163 - val_accuracy: 0.7279\n",
      "Epoch 25/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.3419 - accuracy: 0.8726\n",
      "Epoch 00025: val_loss did not improve from 0.75971\n",
      "43903/43903 [==============================] - 18s 407us/sample - loss: 0.3417 - accuracy: 0.8726 - val_loss: 0.8289 - val_accuracy: 0.7300\n",
      "... Validating on Fold 4 ...\n",
      "Train on 43903 samples, validate on 10976 samples\n",
      "Epoch 1/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 1.3686 - accuracy: 0.4041\n",
      "Epoch 00001: val_loss improved from inf to 1.19032, saving model to ./storage/writer_trainfiles/kfold4/epoch_001_val_1.190.h5\n",
      "43903/43903 [==============================] - 20s 464us/sample - loss: 1.3683 - accuracy: 0.4040 - val_loss: 1.1903 - val_accuracy: 0.5047\n",
      "Epoch 2/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 1.0479 - accuracy: 0.5742\n",
      "Epoch 00002: val_loss improved from 1.19032 to 0.95368, saving model to ./storage/writer_trainfiles/kfold4/epoch_002_val_0.954.h5\n",
      "43903/43903 [==============================] - 18s 418us/sample - loss: 1.0475 - accuracy: 0.5744 - val_loss: 0.9537 - val_accuracy: 0.6224\n",
      "Epoch 3/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.8638 - accuracy: 0.6623\n",
      "Epoch 00003: val_loss improved from 0.95368 to 0.88757, saving model to ./storage/writer_trainfiles/kfold4/epoch_003_val_0.888.h5\n",
      "43903/43903 [==============================] - 18s 421us/sample - loss: 0.8638 - accuracy: 0.6623 - val_loss: 0.8876 - val_accuracy: 0.6598\n",
      "Epoch 4/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.7674 - accuracy: 0.7077\n",
      "Epoch 00004: val_loss improved from 0.88757 to 0.84629, saving model to ./storage/writer_trainfiles/kfold4/epoch_004_val_0.846.h5\n",
      "43903/43903 [==============================] - 19s 423us/sample - loss: 0.7673 - accuracy: 0.7077 - val_loss: 0.8463 - val_accuracy: 0.6773\n",
      "Epoch 5/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.6968 - accuracy: 0.7377\n",
      "Epoch 00005: val_loss did not improve from 0.84629\n",
      "43903/43903 [==============================] - 18s 409us/sample - loss: 0.6969 - accuracy: 0.7378 - val_loss: 0.8568 - val_accuracy: 0.6860\n",
      "Epoch 6/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.6464 - accuracy: 0.7570\n",
      "Epoch 00006: val_loss improved from 0.84629 to 0.80621, saving model to ./storage/writer_trainfiles/kfold4/epoch_006_val_0.806.h5\n",
      "43903/43903 [==============================] - 18s 415us/sample - loss: 0.6464 - accuracy: 0.7570 - val_loss: 0.8062 - val_accuracy: 0.6961\n",
      "Epoch 7/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.6024 - accuracy: 0.7754\n",
      "Epoch 00007: val_loss improved from 0.80621 to 0.80076, saving model to ./storage/writer_trainfiles/kfold4/epoch_007_val_0.801.h5\n",
      "43903/43903 [==============================] - 18s 420us/sample - loss: 0.6024 - accuracy: 0.7754 - val_loss: 0.8008 - val_accuracy: 0.7010\n",
      "Epoch 8/250\n",
      "43840/43903 [============================>.] - ETA: 0s - loss: 0.5749 - accuracy: 0.7879\n",
      "Epoch 00008: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 407us/sample - loss: 0.5747 - accuracy: 0.7880 - val_loss: 0.8181 - val_accuracy: 0.7026\n",
      "Epoch 9/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7961\n",
      "Epoch 00009: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 409us/sample - loss: 0.5496 - accuracy: 0.7960 - val_loss: 0.8082 - val_accuracy: 0.6994\n",
      "Epoch 10/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.5280 - accuracy: 0.8032\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 405us/sample - loss: 0.5279 - accuracy: 0.8033 - val_loss: 0.8085 - val_accuracy: 0.7075\n",
      "Epoch 11/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.4949 - accuracy: 0.8154\n",
      "Epoch 00011: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 409us/sample - loss: 0.4951 - accuracy: 0.8153 - val_loss: 0.8248 - val_accuracy: 0.7049\n",
      "Epoch 12/250\n",
      "43744/43903 [============================>.] - ETA: 0s - loss: 0.4762 - accuracy: 0.8217\n",
      "Epoch 00012: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 407us/sample - loss: 0.4763 - accuracy: 0.8217 - val_loss: 0.8205 - val_accuracy: 0.7081\n",
      "Epoch 13/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.8305\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.4593 - accuracy: 0.8304 - val_loss: 0.8376 - val_accuracy: 0.7108\n",
      "Epoch 14/250\n",
      "43872/43903 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.8363\n",
      "Epoch 00014: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 407us/sample - loss: 0.4387 - accuracy: 0.8363 - val_loss: 0.8587 - val_accuracy: 0.7031\n",
      "Epoch 15/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.4296 - accuracy: 0.8389\n",
      "Epoch 00015: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 404us/sample - loss: 0.4296 - accuracy: 0.8389 - val_loss: 0.8101 - val_accuracy: 0.7152\n",
      "Epoch 16/250\n",
      "43808/43903 [============================>.] - ETA: 0s - loss: 0.4150 - accuracy: 0.8458\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 406us/sample - loss: 0.4150 - accuracy: 0.8458 - val_loss: 0.8344 - val_accuracy: 0.7155\n",
      "Epoch 17/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3973 - accuracy: 0.8520\n",
      "Epoch 00017: val_loss did not improve from 0.80076\n",
      "43903/43903 [==============================] - 18s 405us/sample - loss: 0.3972 - accuracy: 0.8520 - val_loss: 0.8621 - val_accuracy: 0.7158\n",
      "... Validating on Fold 5 ...\n",
      "Train on 43904 samples, validate on 10975 samples\n",
      "Epoch 1/250\n",
      "43744/43904 [============================>.] - ETA: 0s - loss: 1.3842 - accuracy: 0.3893\n",
      "Epoch 00001: val_loss improved from inf to 1.21125, saving model to ./storage/writer_trainfiles/kfold5/epoch_001_val_1.211.h5\n",
      "43904/43904 [==============================] - 21s 477us/sample - loss: 1.3840 - accuracy: 0.3895 - val_loss: 1.2113 - val_accuracy: 0.4869\n",
      "Epoch 2/250\n",
      "43808/43904 [============================>.] - ETA: 0s - loss: 1.0855 - accuracy: 0.5557\n",
      "Epoch 00002: val_loss improved from 1.21125 to 0.96039, saving model to ./storage/writer_trainfiles/kfold5/epoch_002_val_0.960.h5\n",
      "43904/43904 [==============================] - 18s 408us/sample - loss: 1.0854 - accuracy: 0.5557 - val_loss: 0.9604 - val_accuracy: 0.6265\n",
      "Epoch 3/250\n",
      "43840/43904 [============================>.] - ETA: 0s - loss: 0.8833 - accuracy: 0.6539\n",
      "Epoch 00003: val_loss improved from 0.96039 to 0.86520, saving model to ./storage/writer_trainfiles/kfold5/epoch_003_val_0.865.h5\n",
      "43904/43904 [==============================] - 18s 407us/sample - loss: 0.8831 - accuracy: 0.6539 - val_loss: 0.8652 - val_accuracy: 0.6703\n",
      "Epoch 4/250\n",
      "43872/43904 [============================>.] - ETA: 0s - loss: 0.7794 - accuracy: 0.7015\n",
      "Epoch 00004: val_loss improved from 0.86520 to 0.81723, saving model to ./storage/writer_trainfiles/kfold5/epoch_004_val_0.817.h5\n",
      "43904/43904 [==============================] - 18s 415us/sample - loss: 0.7792 - accuracy: 0.7016 - val_loss: 0.8172 - val_accuracy: 0.6961\n",
      "Epoch 5/250\n",
      "43872/43904 [============================>.] - ETA: 0s - loss: 0.7013 - accuracy: 0.7358\n",
      "Epoch 00005: val_loss improved from 0.81723 to 0.78957, saving model to ./storage/writer_trainfiles/kfold5/epoch_005_val_0.790.h5\n",
      "43904/43904 [==============================] - 18s 415us/sample - loss: 0.7012 - accuracy: 0.7358 - val_loss: 0.7896 - val_accuracy: 0.7041\n",
      "Epoch 6/250\n",
      "43872/43904 [============================>.] - ETA: 0s - loss: 0.6444 - accuracy: 0.7599\n",
      "Epoch 00006: val_loss improved from 0.78957 to 0.77072, saving model to ./storage/writer_trainfiles/kfold5/epoch_006_val_0.771.h5\n",
      "43904/43904 [==============================] - 18s 408us/sample - loss: 0.6443 - accuracy: 0.7599 - val_loss: 0.7707 - val_accuracy: 0.7146\n",
      "Epoch 7/250\n",
      "43840/43904 [============================>.] - ETA: 0s - loss: 0.6088 - accuracy: 0.7728\n",
      "Epoch 00007: val_loss did not improve from 0.77072\n",
      "43904/43904 [==============================] - 18s 401us/sample - loss: 0.6088 - accuracy: 0.7728 - val_loss: 0.7800 - val_accuracy: 0.7207\n",
      "Epoch 8/250\n",
      "43872/43904 [============================>.] - ETA: 0s - loss: 0.5757 - accuracy: 0.7861\n",
      "Epoch 00008: val_loss improved from 0.77072 to 0.76540, saving model to ./storage/writer_trainfiles/kfold5/epoch_008_val_0.765.h5\n",
      "43904/43904 [==============================] - 18s 412us/sample - loss: 0.5759 - accuracy: 0.7860 - val_loss: 0.7654 - val_accuracy: 0.7245\n",
      "Epoch 9/250\n",
      "43840/43904 [============================>.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7946\n",
      "Epoch 00009: val_loss did not improve from 0.76540\n",
      "43904/43904 [==============================] - 18s 402us/sample - loss: 0.5506 - accuracy: 0.7948 - val_loss: 0.7683 - val_accuracy: 0.7240\n",
      "Epoch 10/250\n",
      "43872/43904 [============================>.] - ETA: 0s - loss: 0.5273 - accuracy: 0.8019\n",
      "Epoch 00010: val_loss did not improve from 0.76540\n",
      "43904/43904 [==============================] - 18s 400us/sample - loss: 0.5273 - accuracy: 0.8019 - val_loss: 0.7695 - val_accuracy: 0.7252\n",
      "Epoch 11/250\n",
      "43808/43904 [============================>.] - ETA: 0s - loss: 0.5095 - accuracy: 0.8087\n",
      "Epoch 00011: val_loss improved from 0.76540 to 0.75388, saving model to ./storage/writer_trainfiles/kfold5/epoch_011_val_0.754.h5\n",
      "43904/43904 [==============================] - 18s 408us/sample - loss: 0.5094 - accuracy: 0.8088 - val_loss: 0.7539 - val_accuracy: 0.7264\n",
      "Epoch 12/250\n",
      "43840/43904 [============================>.] - ETA: 0s - loss: 0.4872 - accuracy: 0.8176\n",
      "Epoch 00012: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 18s 403us/sample - loss: 0.4871 - accuracy: 0.8177 - val_loss: 0.7574 - val_accuracy: 0.7270\n",
      "Epoch 13/250\n",
      "43776/43904 [============================>.] - ETA: 0s - loss: 0.4770 - accuracy: 0.8242\n",
      "Epoch 00013: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 18s 401us/sample - loss: 0.4774 - accuracy: 0.8241 - val_loss: 0.7915 - val_accuracy: 0.7226\n",
      "Epoch 14/250\n",
      "43840/43904 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.8277\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 18s 402us/sample - loss: 0.4608 - accuracy: 0.8278 - val_loss: 0.7854 - val_accuracy: 0.7250\n",
      "Epoch 15/250\n",
      "43744/43904 [============================>.] - ETA: 0s - loss: 0.4420 - accuracy: 0.8362\n",
      "Epoch 00015: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 18s 400us/sample - loss: 0.4416 - accuracy: 0.8363 - val_loss: 0.7797 - val_accuracy: 0.7331\n",
      "Epoch 16/250\n",
      "43808/43904 [============================>.] - ETA: 0s - loss: 0.4238 - accuracy: 0.8425\n",
      "Epoch 00016: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 17s 397us/sample - loss: 0.4238 - accuracy: 0.8425 - val_loss: 0.7791 - val_accuracy: 0.7317\n",
      "Epoch 17/250\n",
      "43872/43904 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8457\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 18s 402us/sample - loss: 0.4173 - accuracy: 0.8456 - val_loss: 0.7955 - val_accuracy: 0.7338\n",
      "Epoch 18/250\n",
      "43872/43904 [============================>.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8526\n",
      "Epoch 00018: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 18s 402us/sample - loss: 0.4000 - accuracy: 0.8527 - val_loss: 0.7859 - val_accuracy: 0.7350\n",
      "Epoch 19/250\n",
      "43872/43904 [============================>.] - ETA: 0s - loss: 0.3834 - accuracy: 0.8573\n",
      "Epoch 00019: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 18s 399us/sample - loss: 0.3832 - accuracy: 0.8573 - val_loss: 0.8196 - val_accuracy: 0.7274\n",
      "Epoch 20/250\n",
      "43744/43904 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.8565\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 18s 403us/sample - loss: 0.3828 - accuracy: 0.8566 - val_loss: 0.8008 - val_accuracy: 0.7356\n",
      "Epoch 21/250\n",
      "43744/43904 [============================>.] - ETA: 0s - loss: 0.3656 - accuracy: 0.8637\n",
      "Epoch 00021: val_loss did not improve from 0.75388\n",
      "43904/43904 [==============================] - 18s 401us/sample - loss: 0.3656 - accuracy: 0.8637 - val_loss: 0.8055 - val_accuracy: 0.7372\n"
     ]
    }
   ],
   "source": [
    "# conduct KFold Ensemble  \n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 777) \n",
    "for idx, (train_idx,val_idx) in enumerate(kfold.split(train_padded, y_train)):\n",
    "    \n",
    "    print(\"... Validating on Fold {} ...\".format(idx+1))\n",
    "    \n",
    "    # split data into train and validation sets \n",
    "    cur_x_train, cur_x_val = train_padded[train_idx], train_padded[val_idx] \n",
    "    cur_y_train, cur_y_val = y_train[train_idx], y_train[val_idx] \n",
    "    \n",
    "    # build model, define callbacks and train  \n",
    "    model_path = './storage/writer_trainfiles/kfold' + str(idx+1) + '/epoch_{epoch:03d}_val_{val_loss:.3f}.h5'\n",
    "    model = simple_lstm() \n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 3, verbose = 1, factor = 0.8)\n",
    "    checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10) \n",
    "    history = model.fit(cur_x_train,\n",
    "                        cur_y_train,\n",
    "                        validation_data = (cur_x_val,cur_y_val),\n",
    "                        shuffle = True,\n",
    "                        epochs = 250,\n",
    "                        verbose = 1,\n",
    "                        callbacks = [learning_rate_reduction, checkpoint, early_stopping]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('./storage/writer_trainfiles/kfold1/epoch_011_val_0.771.h5')\n",
    "model2 = load_model('./storage/writer_trainfiles/kfold2/epoch_013_val_0.767.h5')\n",
    "model3 = load_model('./storage/writer_trainfiles/kfold3/epoch_015_val_0.760.h5')\n",
    "model4 = load_model('./storage/writer_trainfiles/kfold4/epoch_007_val_0.801.h5')\n",
    "model5 = load_model('./storage/writer_trainfiles/kfold5/epoch_011_val_0.754.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict_proba(test_padded)\n",
    "pred2 = model2.predict_proba(test_padded) \n",
    "pred3 = model3.predict_proba(test_padded) \n",
    "pred4 = model4.predict_proba(test_padded) \n",
    "pred5 = model5.predict_proba(test_padded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_avg = (pred1 + pred2 + pred3 + pred4 + pred5)/5.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004410</td>\n",
       "      <td>0.074918</td>\n",
       "      <td>0.510920</td>\n",
       "      <td>0.407275</td>\n",
       "      <td>0.002477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.183829</td>\n",
       "      <td>0.719099</td>\n",
       "      <td>0.032944</td>\n",
       "      <td>0.042305</td>\n",
       "      <td>0.021822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.984772</td>\n",
       "      <td>0.012736</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.001623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.894060</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.097524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.512074</td>\n",
       "      <td>0.113390</td>\n",
       "      <td>0.130675</td>\n",
       "      <td>0.123092</td>\n",
       "      <td>0.120770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         0         1         2         3         4\n",
       "0      0  0.004410  0.074918  0.510920  0.407275  0.002477\n",
       "1      1  0.183829  0.719099  0.032944  0.042305  0.021822\n",
       "2      2  0.984772  0.012736  0.000375  0.000495  0.001623\n",
       "3      3  0.002930  0.001710  0.894060  0.003775  0.097524\n",
       "4      4  0.512074  0.113390  0.130675  0.123092  0.120770"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss[['0','1','2','3','4']] = pred_avg \n",
    "ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('./storage/initial_submit.csv', index = False, encoding = 'utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
