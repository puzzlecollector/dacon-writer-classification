{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re \n",
    "\n",
    "import nltk # for stopwords \n",
    "from nltk.corpus import stopwords\n",
    "import gensim # for Word2Vec embeddings \n",
    "from gensim.models import KeyedVectors\n",
    "from sentencepiece import SentencePieceTrainer,SentencePieceProcessor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import googletrans \n",
    "from googletrans import Translator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./storage/writer/train.csv') \n",
    "test = pd.read_csv('./storage/writer/test_x.csv') \n",
    "ss = pd.read_csv('./storage/writer/sample_submission.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train['text'] \n",
    "y_train = train['author'] \n",
    "x_test = test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train) \n",
    "y_train = np.asarray(y_train) \n",
    "x_test = np.asarray(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54879,), (54879,), (19617,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_translated = np.load('./storage/sample_back_translate.npy') \n",
    "\n",
    "back_translated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 25000\n",
    "maxlength = 150 \n",
    "embedding_dim = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlength),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 150, 30)           750000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 150, 30)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 150, 256)          162816    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 150, 64)           49216     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 25)                1625      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 130       \n",
      "=================================================================\n",
      "Total params: 963,787\n",
      "Trainable params: 963,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_recurrent_model():  \n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length = maxlength), \n",
    "        Dropout(0.5), \n",
    "        Bidirectional(LSTM(128, return_sequences = True)), \n",
    "        Conv1D(64, 3, padding = 'same', activation = 'relu'), \n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(25, activation = 'relu'), \n",
    "        Dense(5, activation = 'softmax') \n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "model = build_recurrent_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Iteration 1 ...\n",
      "... Preprocessing Data ... \n",
      "(98782, 150) (98782,)\n",
      "(5488, 150) (5488,)\n",
      "... Training Model by Validating on Fold 1 ...\n",
      "Train on 98782 samples, validate on 5488 samples\n",
      "Epoch 1/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 1.2174 - accuracy: 0.4827\n",
      "Epoch 00001: val_loss improved from inf to 0.78747, saving model to ./storage/writer_train_2_10/kfold1/epoch_001_val_0.787.h5\n",
      "98782/98782 [==============================] - 20s 200us/sample - loss: 1.2164 - accuracy: 0.4832 - val_loss: 0.7875 - val_accuracy: 0.6975\n",
      "Epoch 2/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.7833 - accuracy: 0.6992\n",
      "Epoch 00002: val_loss improved from 0.78747 to 0.64211, saving model to ./storage/writer_train_2_10/kfold1/epoch_002_val_0.642.h5\n",
      "98782/98782 [==============================] - 14s 138us/sample - loss: 0.7831 - accuracy: 0.6993 - val_loss: 0.6421 - val_accuracy: 0.7600\n",
      "Epoch 3/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.6351 - accuracy: 0.7633\n",
      "Epoch 00003: val_loss improved from 0.64211 to 0.60174, saving model to ./storage/writer_train_2_10/kfold1/epoch_003_val_0.602.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.6348 - accuracy: 0.7633 - val_loss: 0.6017 - val_accuracy: 0.7801\n",
      "Epoch 4/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5578 - accuracy: 0.7924\n",
      "Epoch 00004: val_loss improved from 0.60174 to 0.58828, saving model to ./storage/writer_train_2_10/kfold1/epoch_004_val_0.588.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.5579 - accuracy: 0.7922 - val_loss: 0.5883 - val_accuracy: 0.7817\n",
      "Epoch 5/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5089 - accuracy: 0.8100\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58828\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.5090 - accuracy: 0.8100 - val_loss: 0.6031 - val_accuracy: 0.7801\n",
      "Epoch 6/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4575 - accuracy: 0.8292\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58828\n",
      "98782/98782 [==============================] - 12s 126us/sample - loss: 0.4575 - accuracy: 0.8291 - val_loss: 0.6073 - val_accuracy: 0.7813\n",
      "Epoch 7/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4289 - accuracy: 0.8401\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58828\n",
      "98782/98782 [==============================] - 12s 126us/sample - loss: 0.4289 - accuracy: 0.8401 - val_loss: 0.6116 - val_accuracy: 0.7843\n",
      "Epoch 8/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4141 - accuracy: 0.8456\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58828\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4139 - accuracy: 0.8457 - val_loss: 0.6226 - val_accuracy: 0.7855\n",
      "... Iteration 2 ...\n",
      "... Preprocessing Data ... \n",
      "(98782, 150) (98782,)\n",
      "(5488, 150) (5488,)\n",
      "... Training Model by Validating on Fold 2 ...\n",
      "Train on 98782 samples, validate on 5488 samples\n",
      "Epoch 1/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 1.1792 - accuracy: 0.5062\n",
      "Epoch 00001: val_loss improved from inf to 0.79925, saving model to ./storage/writer_train_2_10/kfold2/epoch_001_val_0.799.h5\n",
      "98782/98782 [==============================] - 18s 179us/sample - loss: 1.1785 - accuracy: 0.5066 - val_loss: 0.7992 - val_accuracy: 0.6962\n",
      "Epoch 2/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.7778 - accuracy: 0.7030\n",
      "Epoch 00002: val_loss improved from 0.79925 to 0.65952, saving model to ./storage/writer_train_2_10/kfold2/epoch_002_val_0.660.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.7777 - accuracy: 0.7030 - val_loss: 0.6595 - val_accuracy: 0.7540\n",
      "Epoch 3/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.6403 - accuracy: 0.7598\n",
      "Epoch 00003: val_loss improved from 0.65952 to 0.61864, saving model to ./storage/writer_train_2_10/kfold2/epoch_003_val_0.619.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.6405 - accuracy: 0.7597 - val_loss: 0.6186 - val_accuracy: 0.7693\n",
      "Epoch 4/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5680 - accuracy: 0.7866\n",
      "Epoch 00004: val_loss improved from 0.61864 to 0.61039, saving model to ./storage/writer_train_2_10/kfold2/epoch_004_val_0.610.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.5681 - accuracy: 0.7866 - val_loss: 0.6104 - val_accuracy: 0.7801\n",
      "Epoch 5/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5188 - accuracy: 0.8055\n",
      "Epoch 00005: val_loss improved from 0.61039 to 0.60482, saving model to ./storage/writer_train_2_10/kfold2/epoch_005_val_0.605.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.5187 - accuracy: 0.8056 - val_loss: 0.6048 - val_accuracy: 0.7833\n",
      "Epoch 6/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4824 - accuracy: 0.8198\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.60482\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4822 - accuracy: 0.8198 - val_loss: 0.6218 - val_accuracy: 0.7744\n",
      "Epoch 7/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4363 - accuracy: 0.8363\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.60482\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.4362 - accuracy: 0.8364 - val_loss: 0.6179 - val_accuracy: 0.7844\n",
      "Epoch 8/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4078 - accuracy: 0.8476\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.60482\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.4078 - accuracy: 0.8475 - val_loss: 0.6389 - val_accuracy: 0.7802\n",
      "Epoch 9/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8529\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.60482\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.3941 - accuracy: 0.8529 - val_loss: 0.6403 - val_accuracy: 0.7841\n",
      "... Iteration 3 ...\n",
      "... Preprocessing Data ... \n",
      "(98782, 150) (98782,)\n",
      "(5488, 150) (5488,)\n",
      "... Training Model by Validating on Fold 3 ...\n",
      "Train on 98782 samples, validate on 5488 samples\n",
      "Epoch 1/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 1.1890 - accuracy: 0.5077\n",
      "Epoch 00001: val_loss improved from inf to 0.76260, saving model to ./storage/writer_train_2_10/kfold3/epoch_001_val_0.763.h5\n",
      "98782/98782 [==============================] - 18s 180us/sample - loss: 1.1883 - accuracy: 0.5080 - val_loss: 0.7626 - val_accuracy: 0.7136\n",
      "Epoch 2/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.7588 - accuracy: 0.7102\n",
      "Epoch 00002: val_loss improved from 0.76260 to 0.64443, saving model to ./storage/writer_train_2_10/kfold3/epoch_002_val_0.644.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.7587 - accuracy: 0.7102 - val_loss: 0.6444 - val_accuracy: 0.7582\n",
      "Epoch 3/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.6246 - accuracy: 0.7639\n",
      "Epoch 00003: val_loss improved from 0.64443 to 0.63032, saving model to ./storage/writer_train_2_10/kfold3/epoch_003_val_0.630.h5\n",
      "98782/98782 [==============================] - 14s 140us/sample - loss: 0.6246 - accuracy: 0.7639 - val_loss: 0.6303 - val_accuracy: 0.7657\n",
      "Epoch 4/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5500 - accuracy: 0.7932\n",
      "Epoch 00004: val_loss improved from 0.63032 to 0.58494, saving model to ./storage/writer_train_2_10/kfold3/epoch_004_val_0.585.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.5499 - accuracy: 0.7932 - val_loss: 0.5849 - val_accuracy: 0.7843\n",
      "Epoch 5/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5043 - accuracy: 0.8108\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58494\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.5042 - accuracy: 0.8107 - val_loss: 0.6191 - val_accuracy: 0.7753\n",
      "Epoch 6/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4541 - accuracy: 0.8310\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58494\n",
      "98782/98782 [==============================] - 13s 129us/sample - loss: 0.4541 - accuracy: 0.8310 - val_loss: 0.6022 - val_accuracy: 0.7866\n",
      "Epoch 7/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4212 - accuracy: 0.8413\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58494\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4213 - accuracy: 0.8413 - val_loss: 0.6194 - val_accuracy: 0.7859\n",
      "Epoch 8/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4061 - accuracy: 0.8476\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58494\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.4061 - accuracy: 0.8476 - val_loss: 0.6192 - val_accuracy: 0.7879\n",
      "... Iteration 4 ...\n",
      "... Preprocessing Data ... \n",
      "(98782, 150) (98782,)\n",
      "(5488, 150) (5488,)\n",
      "... Training Model by Validating on Fold 4 ...\n",
      "Train on 98782 samples, validate on 5488 samples\n",
      "Epoch 1/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 1.1688 - accuracy: 0.5129\n",
      "Epoch 00001: val_loss improved from inf to 0.82311, saving model to ./storage/writer_train_2_10/kfold4/epoch_001_val_0.823.h5\n",
      "98782/98782 [==============================] - 17s 177us/sample - loss: 1.1681 - accuracy: 0.5132 - val_loss: 0.8231 - val_accuracy: 0.6837\n",
      "Epoch 2/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.7836 - accuracy: 0.6999\n",
      "Epoch 00002: val_loss improved from 0.82311 to 0.65014, saving model to ./storage/writer_train_2_10/kfold4/epoch_002_val_0.650.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.7835 - accuracy: 0.7000 - val_loss: 0.6501 - val_accuracy: 0.7648\n",
      "Epoch 3/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.6298 - accuracy: 0.7639\n",
      "Epoch 00003: val_loss improved from 0.65014 to 0.60044, saving model to ./storage/writer_train_2_10/kfold4/epoch_003_val_0.600.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.6297 - accuracy: 0.7639 - val_loss: 0.6004 - val_accuracy: 0.7806\n",
      "Epoch 4/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5485 - accuracy: 0.7964\n",
      "Epoch 00004: val_loss improved from 0.60044 to 0.59826, saving model to ./storage/writer_train_2_10/kfold4/epoch_004_val_0.598.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.5484 - accuracy: 0.7965 - val_loss: 0.5983 - val_accuracy: 0.7813\n",
      "Epoch 5/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5063 - accuracy: 0.8107\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59826\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.5063 - accuracy: 0.8107 - val_loss: 0.6062 - val_accuracy: 0.7802\n",
      "Epoch 6/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4492 - accuracy: 0.8321\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59826\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.4493 - accuracy: 0.8321 - val_loss: 0.6130 - val_accuracy: 0.7833\n",
      "Epoch 7/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4179 - accuracy: 0.8435\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.59826\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.4179 - accuracy: 0.8435 - val_loss: 0.6150 - val_accuracy: 0.7868\n",
      "Epoch 8/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4030 - accuracy: 0.8496\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.59826\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4031 - accuracy: 0.8496 - val_loss: 0.6207 - val_accuracy: 0.7874\n",
      "... Iteration 5 ...\n",
      "... Preprocessing Data ... \n",
      "(98782, 150) (98782,)\n",
      "(5488, 150) (5488,)\n",
      "... Training Model by Validating on Fold 5 ...\n",
      "Train on 98782 samples, validate on 5488 samples\n",
      "Epoch 1/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 1.2048 - accuracy: 0.4909\n",
      "Epoch 00001: val_loss improved from inf to 0.81386, saving model to ./storage/writer_train_2_10/kfold5/epoch_001_val_0.814.h5\n",
      "98782/98782 [==============================] - 18s 178us/sample - loss: 1.2043 - accuracy: 0.4912 - val_loss: 0.8139 - val_accuracy: 0.6860\n",
      "Epoch 2/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.7890 - accuracy: 0.6983\n",
      "Epoch 00002: val_loss improved from 0.81386 to 0.66358, saving model to ./storage/writer_train_2_10/kfold5/epoch_002_val_0.664.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.7890 - accuracy: 0.6983 - val_loss: 0.6636 - val_accuracy: 0.7555\n",
      "Epoch 3/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.6449 - accuracy: 0.7575\n",
      "Epoch 00003: val_loss improved from 0.66358 to 0.60403, saving model to ./storage/writer_train_2_10/kfold5/epoch_003_val_0.604.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.6451 - accuracy: 0.7575 - val_loss: 0.6040 - val_accuracy: 0.7795\n",
      "Epoch 4/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5737 - accuracy: 0.7865\n",
      "Epoch 00004: val_loss improved from 0.60403 to 0.59178, saving model to ./storage/writer_train_2_10/kfold5/epoch_004_val_0.592.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.5735 - accuracy: 0.7866 - val_loss: 0.5918 - val_accuracy: 0.7817\n",
      "Epoch 5/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5229 - accuracy: 0.8041\n",
      "Epoch 00005: val_loss improved from 0.59178 to 0.58995, saving model to ./storage/writer_train_2_10/kfold5/epoch_005_val_0.590.h5\n",
      "98782/98782 [==============================] - 14s 137us/sample - loss: 0.5226 - accuracy: 0.8042 - val_loss: 0.5899 - val_accuracy: 0.7790\n",
      "Epoch 6/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4940 - accuracy: 0.8162\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58995\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4942 - accuracy: 0.8161 - val_loss: 0.6083 - val_accuracy: 0.7768\n",
      "Epoch 7/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4464 - accuracy: 0.8342\n",
      "Epoch 00007: val_loss improved from 0.58995 to 0.58926, saving model to ./storage/writer_train_2_10/kfold5/epoch_007_val_0.589.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.4463 - accuracy: 0.8342 - val_loss: 0.5893 - val_accuracy: 0.7903\n",
      "Epoch 8/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4300 - accuracy: 0.8397\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58926\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4300 - accuracy: 0.8397 - val_loss: 0.6018 - val_accuracy: 0.7861\n",
      "Epoch 9/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4052 - accuracy: 0.8482\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.58926\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4050 - accuracy: 0.8483 - val_loss: 0.6165 - val_accuracy: 0.7855\n",
      "Epoch 10/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.3935 - accuracy: 0.8528 ETA: 0s - los\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.58926\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.3932 - accuracy: 0.8529 - val_loss: 0.6257 - val_accuracy: 0.7853\n",
      "Epoch 11/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.8553\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.58926\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.3867 - accuracy: 0.8553 - val_loss: 0.6248 - val_accuracy: 0.7863\n",
      "... Iteration 6 ...\n",
      "... Preprocessing Data ... \n",
      "(98782, 150) (98782,)\n",
      "(5488, 150) (5488,)\n",
      "... Training Model by Validating on Fold 6 ...\n",
      "Train on 98782 samples, validate on 5488 samples\n",
      "Epoch 1/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 1.1953 - accuracy: 0.5000\n",
      "Epoch 00001: val_loss improved from inf to 0.75091, saving model to ./storage/writer_train_2_10/kfold6/epoch_001_val_0.751.h5\n",
      "98782/98782 [==============================] - 18s 179us/sample - loss: 1.1947 - accuracy: 0.5004 - val_loss: 0.7509 - val_accuracy: 0.7183\n",
      "Epoch 2/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.7560 - accuracy: 0.7111\n",
      "Epoch 00002: val_loss improved from 0.75091 to 0.62351, saving model to ./storage/writer_train_2_10/kfold6/epoch_002_val_0.624.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.7559 - accuracy: 0.7113 - val_loss: 0.6235 - val_accuracy: 0.7657\n",
      "Epoch 3/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.6259 - accuracy: 0.7643\n",
      "Epoch 00003: val_loss improved from 0.62351 to 0.58432, saving model to ./storage/writer_train_2_10/kfold6/epoch_003_val_0.584.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.6259 - accuracy: 0.7643 - val_loss: 0.5843 - val_accuracy: 0.7861\n",
      "Epoch 4/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5542 - accuracy: 0.7926\n",
      "Epoch 00004: val_loss improved from 0.58432 to 0.58070, saving model to ./storage/writer_train_2_10/kfold6/epoch_004_val_0.581.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.5543 - accuracy: 0.7925 - val_loss: 0.5807 - val_accuracy: 0.7925\n",
      "Epoch 5/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5110 - accuracy: 0.8074\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58070\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.5109 - accuracy: 0.8075 - val_loss: 0.5846 - val_accuracy: 0.7897\n",
      "Epoch 6/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4566 - accuracy: 0.8291\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58070\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4567 - accuracy: 0.8290 - val_loss: 0.5873 - val_accuracy: 0.7956\n",
      "Epoch 7/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8406\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58070\n",
      "98782/98782 [==============================] - 12s 127us/sample - loss: 0.4266 - accuracy: 0.8407 - val_loss: 0.5990 - val_accuracy: 0.7999\n",
      "Epoch 8/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.8468\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58070\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4093 - accuracy: 0.8468 - val_loss: 0.6020 - val_accuracy: 0.7997\n",
      "... Iteration 7 ...\n",
      "... Preprocessing Data ... \n",
      "(98782, 150) (98782,)\n",
      "(5488, 150) (5488,)\n",
      "... Training Model by Validating on Fold 7 ...\n",
      "Train on 98782 samples, validate on 5488 samples\n",
      "Epoch 1/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 1.2873 - accuracy: 0.4610\n",
      "Epoch 00001: val_loss improved from inf to 0.89868, saving model to ./storage/writer_train_2_10/kfold7/epoch_001_val_0.899.h5\n",
      "98782/98782 [==============================] - 18s 181us/sample - loss: 1.2866 - accuracy: 0.4613 - val_loss: 0.8987 - val_accuracy: 0.6543\n",
      "Epoch 2/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.8213 - accuracy: 0.6844\n",
      "Epoch 00002: val_loss improved from 0.89868 to 0.73568, saving model to ./storage/writer_train_2_10/kfold7/epoch_002_val_0.736.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.8210 - accuracy: 0.6846 - val_loss: 0.7357 - val_accuracy: 0.7232\n",
      "Epoch 3/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.6722 - accuracy: 0.7468\n",
      "Epoch 00003: val_loss improved from 0.73568 to 0.67520, saving model to ./storage/writer_train_2_10/kfold7/epoch_003_val_0.675.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.6725 - accuracy: 0.7467 - val_loss: 0.6752 - val_accuracy: 0.7464\n",
      "Epoch 4/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5913 - accuracy: 0.7789\n",
      "Epoch 00004: val_loss improved from 0.67520 to 0.65396, saving model to ./storage/writer_train_2_10/kfold7/epoch_004_val_0.654.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.5912 - accuracy: 0.7789 - val_loss: 0.6540 - val_accuracy: 0.7620\n",
      "Epoch 5/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5398 - accuracy: 0.7967\n",
      "Epoch 00005: val_loss improved from 0.65396 to 0.64796, saving model to ./storage/writer_train_2_10/kfold7/epoch_005_val_0.648.h5\n",
      "98782/98782 [==============================] - 13s 134us/sample - loss: 0.5399 - accuracy: 0.7967 - val_loss: 0.6480 - val_accuracy: 0.7597\n",
      "Epoch 6/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4975 - accuracy: 0.8141\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.64796\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4974 - accuracy: 0.8141 - val_loss: 0.6494 - val_accuracy: 0.7684\n",
      "Epoch 7/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4507 - accuracy: 0.8311\n",
      "Epoch 00007: val_loss improved from 0.64796 to 0.64634, saving model to ./storage/writer_train_2_10/kfold7/epoch_007_val_0.646.h5\n",
      "98782/98782 [==============================] - 14s 137us/sample - loss: 0.4508 - accuracy: 0.8310 - val_loss: 0.6463 - val_accuracy: 0.7746\n",
      "Epoch 8/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4324 - accuracy: 0.8387\n",
      "Epoch 00008: val_loss improved from 0.64634 to 0.64573, saving model to ./storage/writer_train_2_10/kfold7/epoch_008_val_0.646.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.4325 - accuracy: 0.8386 - val_loss: 0.6457 - val_accuracy: 0.7726\n",
      "Epoch 9/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4190 - accuracy: 0.8432\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.64573\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4190 - accuracy: 0.8432 - val_loss: 0.6718 - val_accuracy: 0.7699\n",
      "Epoch 10/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.3976 - accuracy: 0.8508\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.64573\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.3975 - accuracy: 0.8508 - val_loss: 0.6692 - val_accuracy: 0.7741\n",
      "Epoch 11/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8558\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.64573\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.3854 - accuracy: 0.8558 - val_loss: 0.6746 - val_accuracy: 0.7741\n",
      "Epoch 12/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.3761 - accuracy: 0.8597\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.64573\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.3761 - accuracy: 0.8598 - val_loss: 0.6827 - val_accuracy: 0.7739\n",
      "... Iteration 8 ...\n",
      "... Preprocessing Data ... \n",
      "(98782, 150) (98782,)\n",
      "(5488, 150) (5488,)\n",
      "... Training Model by Validating on Fold 8 ...\n",
      "Train on 98782 samples, validate on 5488 samples\n",
      "Epoch 1/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 1.2048 - accuracy: 0.4879\n",
      "Epoch 00001: val_loss improved from inf to 0.85603, saving model to ./storage/writer_train_2_10/kfold8/epoch_001_val_0.856.h5\n",
      "98782/98782 [==============================] - 18s 180us/sample - loss: 1.2041 - accuracy: 0.4883 - val_loss: 0.8560 - val_accuracy: 0.6662\n",
      "Epoch 2/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.8164 - accuracy: 0.6852\n",
      "Epoch 00002: val_loss improved from 0.85603 to 0.66130, saving model to ./storage/writer_train_2_10/kfold8/epoch_002_val_0.661.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.8163 - accuracy: 0.6852 - val_loss: 0.6613 - val_accuracy: 0.7542\n",
      "Epoch 3/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.6539 - accuracy: 0.7554\n",
      "Epoch 00003: val_loss improved from 0.66130 to 0.59250, saving model to ./storage/writer_train_2_10/kfold8/epoch_003_val_0.593.h5\n",
      "98782/98782 [==============================] - 14s 137us/sample - loss: 0.6539 - accuracy: 0.7554 - val_loss: 0.5925 - val_accuracy: 0.7846\n",
      "Epoch 4/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5690 - accuracy: 0.7865\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59250\n",
      "98782/98782 [==============================] - 13s 129us/sample - loss: 0.5692 - accuracy: 0.7864 - val_loss: 0.5983 - val_accuracy: 0.7850\n",
      "Epoch 5/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4967 - accuracy: 0.8147\n",
      "Epoch 00005: val_loss improved from 0.59250 to 0.59211, saving model to ./storage/writer_train_2_10/kfold8/epoch_005_val_0.592.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.4968 - accuracy: 0.8147 - val_loss: 0.5921 - val_accuracy: 0.7890\n",
      "Epoch 6/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4718 - accuracy: 0.8231\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59211\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.4720 - accuracy: 0.8231 - val_loss: 0.5954 - val_accuracy: 0.7872\n",
      "Epoch 7/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4425 - accuracy: 0.8345\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.59211\n",
      "98782/98782 [==============================] - 13s 129us/sample - loss: 0.4424 - accuracy: 0.8345 - val_loss: 0.6054 - val_accuracy: 0.7875\n",
      "Epoch 8/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4228 - accuracy: 0.8432\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.59211\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4229 - accuracy: 0.8432 - val_loss: 0.6113 - val_accuracy: 0.7864\n",
      "Epoch 9/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4149 - accuracy: 0.8462\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.59211\n",
      "98782/98782 [==============================] - 13s 129us/sample - loss: 0.4148 - accuracy: 0.8462 - val_loss: 0.6176 - val_accuracy: 0.7883\n",
      "... Iteration 9 ...\n",
      "... Preprocessing Data ... \n",
      "(98782, 150) (98782,)\n",
      "(5488, 150) (5488,)\n",
      "... Training Model by Validating on Fold 9 ...\n",
      "Train on 98782 samples, validate on 5488 samples\n",
      "Epoch 1/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 1.1718 - accuracy: 0.5151\n",
      "Epoch 00001: val_loss improved from inf to 0.76789, saving model to ./storage/writer_train_2_10/kfold9/epoch_001_val_0.768.h5\n",
      "98782/98782 [==============================] - 18s 182us/sample - loss: 1.1710 - accuracy: 0.5154 - val_loss: 0.7679 - val_accuracy: 0.7116\n",
      "Epoch 2/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.7647 - accuracy: 0.7071\n",
      "Epoch 00002: val_loss improved from 0.76789 to 0.64753, saving model to ./storage/writer_train_2_10/kfold9/epoch_002_val_0.648.h5\n",
      "98782/98782 [==============================] - 13s 136us/sample - loss: 0.7648 - accuracy: 0.7071 - val_loss: 0.6475 - val_accuracy: 0.7582\n",
      "Epoch 3/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.6391 - accuracy: 0.7583\n",
      "Epoch 00003: val_loss improved from 0.64753 to 0.59983, saving model to ./storage/writer_train_2_10/kfold9/epoch_003_val_0.600.h5\n",
      "98782/98782 [==============================] - 13s 136us/sample - loss: 0.6391 - accuracy: 0.7583 - val_loss: 0.5998 - val_accuracy: 0.7799\n",
      "Epoch 4/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7875\n",
      "Epoch 00004: val_loss improved from 0.59983 to 0.58323, saving model to ./storage/writer_train_2_10/kfold9/epoch_004_val_0.583.h5\n",
      "98782/98782 [==============================] - 13s 135us/sample - loss: 0.5627 - accuracy: 0.7876 - val_loss: 0.5832 - val_accuracy: 0.7872\n",
      "Epoch 5/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.5133 - accuracy: 0.8065\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58323\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.5132 - accuracy: 0.8065 - val_loss: 0.5971 - val_accuracy: 0.7830\n",
      "Epoch 6/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4585 - accuracy: 0.8279\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58323\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4585 - accuracy: 0.8279 - val_loss: 0.5924 - val_accuracy: 0.7914\n",
      "Epoch 7/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4242 - accuracy: 0.8415\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58323\n",
      "98782/98782 [==============================] - 13s 127us/sample - loss: 0.4243 - accuracy: 0.8415 - val_loss: 0.6044 - val_accuracy: 0.7908\n",
      "Epoch 8/250\n",
      "98560/98782 [============================>.] - ETA: 0s - loss: 0.4107 - accuracy: 0.8461\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58323\n",
      "98782/98782 [==============================] - 13s 128us/sample - loss: 0.4106 - accuracy: 0.8461 - val_loss: 0.6080 - val_accuracy: 0.7884\n",
      "... Iteration 10 ...\n",
      "... Preprocessing Data ... \n",
      "(98784, 150) (98784,)\n",
      "(5487, 150) (5487,)\n",
      "... Training Model by Validating on Fold 10 ...\n",
      "Train on 98784 samples, validate on 5487 samples\n",
      "Epoch 1/250\n",
      "98560/98784 [============================>.] - ETA: 0s - loss: 1.2148 - accuracy: 0.4854\n",
      "Epoch 00001: val_loss improved from inf to 0.77213, saving model to ./storage/writer_train_2_10/kfold10/epoch_001_val_0.772.h5\n",
      "98784/98784 [==============================] - 18s 186us/sample - loss: 1.2142 - accuracy: 0.4858 - val_loss: 0.7721 - val_accuracy: 0.7208\n",
      "Epoch 2/250\n",
      "98560/98784 [============================>.] - ETA: 0s - loss: 0.7594 - accuracy: 0.7122\n",
      "Epoch 00002: val_loss improved from 0.77213 to 0.64370, saving model to ./storage/writer_train_2_10/kfold10/epoch_002_val_0.644.h5\n",
      "98784/98784 [==============================] - 13s 135us/sample - loss: 0.7595 - accuracy: 0.7121 - val_loss: 0.6437 - val_accuracy: 0.7611\n",
      "Epoch 3/250\n",
      "98560/98784 [============================>.] - ETA: 0s - loss: 0.6212 - accuracy: 0.7666\n",
      "Epoch 00003: val_loss improved from 0.64370 to 0.60256, saving model to ./storage/writer_train_2_10/kfold10/epoch_003_val_0.603.h5\n",
      "98784/98784 [==============================] - 14s 137us/sample - loss: 0.6213 - accuracy: 0.7666 - val_loss: 0.6026 - val_accuracy: 0.7769\n",
      "Epoch 4/250\n",
      "98560/98784 [============================>.] - ETA: 0s - loss: 0.5475 - accuracy: 0.7946\n",
      "Epoch 00004: val_loss improved from 0.60256 to 0.58115, saving model to ./storage/writer_train_2_10/kfold10/epoch_004_val_0.581.h5\n",
      "98784/98784 [==============================] - 13s 134us/sample - loss: 0.5473 - accuracy: 0.7946 - val_loss: 0.5811 - val_accuracy: 0.7875\n",
      "Epoch 5/250\n",
      "98560/98784 [============================>.] - ETA: 0s - loss: 0.5057 - accuracy: 0.8099\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58115\n",
      "98784/98784 [==============================] - 13s 127us/sample - loss: 0.5059 - accuracy: 0.8098 - val_loss: 0.5856 - val_accuracy: 0.7868\n",
      "Epoch 6/250\n",
      "98560/98784 [============================>.] - ETA: 0s - loss: 0.4507 - accuracy: 0.8316\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58115\n",
      "98784/98784 [==============================] - 13s 127us/sample - loss: 0.4509 - accuracy: 0.8315 - val_loss: 0.5902 - val_accuracy: 0.7893\n",
      "Epoch 7/250\n",
      "98560/98784 [============================>.] - ETA: 0s - loss: 0.4197 - accuracy: 0.8435\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58115\n",
      "98784/98784 [==============================] - 13s 127us/sample - loss: 0.4195 - accuracy: 0.8435 - val_loss: 0.5987 - val_accuracy: 0.7897\n",
      "Epoch 8/250\n",
      "98560/98784 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8477\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58115\n",
      "98784/98784 [==============================] - 13s 127us/sample - loss: 0.4040 - accuracy: 0.8477 - val_loss: 0.6054 - val_accuracy: 0.7922\n"
     ]
    }
   ],
   "source": [
    "# conduct KFold Ensemble  \n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 777) \n",
    "for idx, (train_idx,val_idx) in enumerate(kfold.split(x_train, y_train)):\n",
    "    print(\"... Iteration {} ...\".format(idx+1))   \n",
    "    \n",
    "    print(\"... Preprocessing Data ... \")\n",
    "    \n",
    "    cur_x_train, cur_x_val = x_train[train_idx], x_train[val_idx] \n",
    "    cur_y_train, cur_y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    cur_x_train = np.concatenate([cur_x_train, back_translated[train_idx]]) \n",
    "    cur_y_train = np.concatenate([cur_y_train, cur_y_train])     \n",
    "    \n",
    "    # create tokenizer instance \n",
    "    tokenizer = Tokenizer(num_words = vocab_size, lower = True)\n",
    "    tokenizer.fit_on_texts(cur_x_train) # fit on entire train data \n",
    "    \n",
    "    train_sequences = tokenizer.texts_to_sequences(cur_x_train)\n",
    "    train_padded = pad_sequences(train_sequences, padding='post', maxlen=maxlength)   \n",
    "    \n",
    "    val_sequences = tokenizer.texts_to_sequences(cur_x_val)\n",
    "    val_padded = pad_sequences(val_sequences, padding='post', maxlen=maxlength)\n",
    "    \n",
    "    # create padded sequence for test data  \n",
    "    test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "    test_padded = pad_sequences(test_sequences, padding='post', maxlen=maxlength)\n",
    "    np.save('./storage/test_padded_fold' + str(idx+1) + '.npy', test_padded)\n",
    "    \n",
    "    print(train_padded.shape, cur_y_train.shape) \n",
    "    print(val_padded.shape, cur_y_val.shape)\n",
    "\n",
    "    \n",
    "    print(\"... Training Model by Validating on Fold {} ...\".format(idx+1))\n",
    "\n",
    "    # build model, define callbacks and train  \n",
    "    model_path = './storage/writer_train_2_10/kfold' + str(idx+1) + '/epoch_{epoch:03d}_val_{val_loss:.3f}.h5'\n",
    "    model = build_recurrent_model() \n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 1, verbose = 1, factor = 0.5)\n",
    "    checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 4) \n",
    "    history = model.fit(train_padded,\n",
    "                        cur_y_train,\n",
    "                        validation_data = (val_padded,cur_y_val),\n",
    "                        shuffle = True,\n",
    "                        batch_size = 256, \n",
    "                        epochs = 250,\n",
    "                        verbose = 1,\n",
    "                        callbacks = [learning_rate_reduction, checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('./storage/writer_train_2_10/kfold1/epoch_004_val_0.588.h5') \n",
    "model2 = load_model('./storage/writer_train_2_10/kfold2/epoch_005_val_0.605.h5') \n",
    "model3 = load_model('./storage/writer_train_2_10/kfold3/epoch_004_val_0.585.h5')\n",
    "model4 = load_model('./storage/writer_train_2_10/kfold4/epoch_004_val_0.598.h5')\n",
    "model5 = load_model('./storage/writer_train_2_10/kfold5/epoch_007_val_0.589.h5')\n",
    "model6 = load_model('./storage/writer_train_2_10/kfold6/epoch_004_val_0.581.h5')\n",
    "model7 = load_model('./storage/writer_train_2_10/kfold7/epoch_008_val_0.646.h5') \n",
    "model8 = load_model('./storage/writer_train_2_10/kfold8/epoch_005_val_0.592.h5')\n",
    "model9 = load_model('./storage/writer_train_2_10/kfold9/epoch_004_val_0.583.h5') \n",
    "model10 = load_model('./storage/writer_train_2_10/kfold10/epoch_004_val_0.581.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded1 = np.load('./storage/test_padded_fold1.npy') \n",
    "test_padded2 = np.load('./storage/test_padded_fold2.npy')\n",
    "test_padded3 = np.load('./storage/test_padded_fold3.npy')\n",
    "test_padded4 = np.load('./storage/test_padded_fold4.npy')\n",
    "test_padded5 = np.load('./storage/test_padded_fold5.npy')\n",
    "test_padded6 = np.load('./storage/test_padded_fold6.npy')\n",
    "test_padded7 = np.load('./storage/test_padded_fold7.npy')\n",
    "test_padded8 = np.load('./storage/test_padded_fold8.npy')\n",
    "test_padded9 = np.load('./storage/test_padded_fold9.npy')\n",
    "test_padded10 = np.load('./storage/test_padded_fold10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict_proba(test_padded1)\n",
    "pred2 = model2.predict_proba(test_padded2)\n",
    "pred3 = model3.predict_proba(test_padded3)\n",
    "pred4 = model4.predict_proba(test_padded4)\n",
    "pred5 = model5.predict_proba(test_padded5) \n",
    "pred6 = model6.predict_proba(test_padded6) \n",
    "pred7 = model7.predict_proba(test_padded7) \n",
    "pred8 = model8.predict_proba(test_padded8) \n",
    "pred9 = model9.predict_proba(test_padded9) \n",
    "pred10 = model10.predict_proba(test_padded10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.135789</td>\n",
       "      <td>0.839609</td>\n",
       "      <td>0.020775</td>\n",
       "      <td>0.000926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.166631</td>\n",
       "      <td>0.254356</td>\n",
       "      <td>0.099748</td>\n",
       "      <td>0.338618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.998160</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.924926</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.065943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.650462</td>\n",
       "      <td>0.038764</td>\n",
       "      <td>0.023527</td>\n",
       "      <td>0.234164</td>\n",
       "      <td>0.053083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         0         1         2         3         4\n",
       "0      0  0.002901  0.135789  0.839609  0.020775  0.000926\n",
       "1      1  0.140647  0.166631  0.254356  0.099748  0.338618\n",
       "2      2  0.998160  0.000843  0.000187  0.000187  0.000623\n",
       "3      3  0.006889  0.001724  0.924926  0.000517  0.065943\n",
       "4      4  0.650462  0.038764  0.023527  0.234164  0.053083"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_avg = (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10)/10.0 \n",
    "ss[['0','1','2','3','4']] = pred_avg\n",
    "ss.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('./storage/MarianNMT_augmented_recurrent.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
