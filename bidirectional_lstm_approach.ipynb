{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re \n",
    "\n",
    "import nltk # for stopwords \n",
    "from nltk.corpus import stopwords\n",
    "import gensim # for Word2Vec embeddings \n",
    "from sentencepiece import SentencePieceTrainer,SentencePieceProcessor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./storage/writer/train.csv', encoding = 'utf-8') \n",
    "test = pd.read_csv('./storage/writer/test_x.csv', encoding = 'utf-8') \n",
    "ss = pd.read_csv('./storage/writer/sample_submission.csv', encoding = 'utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 부호 제거하는 함수 \n",
    "def alpha_num(text): \n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  author\n",
       "0      0  He was almost choking. There was so much, so m...       3\n",
       "1      1             “Your sister asked for it, I suppose?”       2\n",
       "2      2   She was engaged one day as she walked, in per...       1\n",
       "3      3  The captain was in the porch, keeping himself ...       4\n",
       "4      4  “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거하는 함수 \n",
    "# we believe this is not necessary \n",
    "stopwords_list = stopwords.words('english')\n",
    "def remove_stopwords(text): \n",
    "    final_text = [] \n",
    "    for i in text.split(): \n",
    "        if i.strip().lower() not in stopwords_list: \n",
    "            final_text.append(i.strip()) \n",
    "    return \" \".join(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "train['text'] = train['text'].str.lower() \n",
    "test['text'] = test['text'].str.lower() \n",
    "train['text'] = train['text'].apply(alpha_num)\n",
    "test['text'] = test['text'].apply(alpha_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([x for x in train['text']]) \n",
    "X_test = np.array([x for x in test['text']])            \n",
    "y_train = np.array([x for x in train['author']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54879,), (54879,), (19617,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec model on our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = gensim.models.Word2Vec(texts, size = 300, min_count = 1, iter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above code snippet, size is the dimension that the Word2Vec vectors will have. min_count is how many encounters are required to add the word in our vocabulary. iter is how many epochs the Word2Vec model should use to learn the semantic correlations (10 in our case) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_model.wv.vocab) + 1, 300)) \n",
    "for i, vec in enumerate(word_model.wv.vectors): \n",
    "    embedding_matrix[i] = vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize our text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(X_train) \n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = len(word_idx) + 1 \n",
    "embedding_dim = 16\n",
    "padding_type='post' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in train_sequences]\n",
    "num_tokens = np.asarray(num_tokens)\n",
    "max_tokens = np.mean(num_tokens) + 2*np.std(num_tokens) \n",
    "max_tokens = int(max_tokens) \n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen = max_tokens, padding = padding_type, truncating = padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(X_test) \n",
    "test_padded = pad_sequences(test_sequences, maxlen = max_tokens, padding = padding_type, truncating = padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54879, 143), (19617, 143))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded.shape, test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try loading Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict() \n",
    "f = open('./storage/glove.6B/glove.6B.300d.txt')\n",
    "for line in f:  \n",
    "    values = line.split() \n",
    "    word = values[0] \n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs \n",
    "f.close() \n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = len(tokenizer.word_index.items())+ 1 \n",
    "embedding_matrix = np.zeros((vocabs, 300))\n",
    "for word, i in tokenizer.word_index.items():  \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:  \n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_gru(): \n",
    "    model = Sequential() \n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length = max_length)) \n",
    "    model.add(Bidirectional(GRU(150, return_sequences = True))) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(100)) \n",
    "    model.add(Dense(32, activation = 'relu', kernel_regularizer = regularizers.l2(0.01)))\n",
    "    model.add(Dense(5, activation = 'softmax')) \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lstm(): \n",
    "    model = Sequential() \n",
    "    model.add(Embedding(47121, 300, weights = [embedding_matrix], input_length = 500, trainable = True)) \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(128, 5, padding = 'valid', activation = 'relu')) \n",
    "    model.add(Conv1D(128, 5, padding = 'valid', activation = 'relu')) \n",
    "    model.add(MaxPooling1D(pool_size = 4))  \n",
    "    model.add(LSTM(55)) \n",
    "    model.add(Dense(5, activation = 'softmax')) \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm():\n",
    "    model = Sequential() \n",
    "    model.add(Embedding(input_dim = features, output_dim = embedding_dim, input_length = max_tokens)) \n",
    "    model.add(Bidirectional(LSTM(16, return_sequences = True, dropout = 0.1, recurrent_dropout = 0.1))) \n",
    "    model.add(Bidirectional(LSTM(8, return_sequences = False))) \n",
    "    model.add(Dropout(0.1)) \n",
    "    model.add(Dense(5, activation = 'softmax')) \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = Adam(lr=0.01), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Validating on Fold 1 ...\n",
      "Train on 43903 samples, validate on 10976 samples\n",
      "Epoch 1/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 1.1928 - accuracy: 0.5198\n",
      "Epoch 00001: val_loss improved from inf to 0.95022, saving model to ./storage/writer_trainfiles2/kfold1/epoch_001_val_0.950.h5\n",
      "43903/43903 [==============================] - 56s 1ms/sample - loss: 1.1923 - accuracy: 0.5201 - val_loss: 0.9502 - val_accuracy: 0.6379\n",
      "Epoch 2/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.7664 - accuracy: 0.7222\n",
      "Epoch 00002: val_loss improved from 0.95022 to 0.78328, saving model to ./storage/writer_trainfiles2/kfold1/epoch_002_val_0.783.h5\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.7666 - accuracy: 0.7221 - val_loss: 0.7833 - val_accuracy: 0.7058\n",
      "Epoch 3/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.5403 - accuracy: 0.8109\n",
      "Epoch 00003: val_loss improved from 0.78328 to 0.76300, saving model to ./storage/writer_trainfiles2/kfold1/epoch_003_val_0.763.h5\n",
      "43903/43903 [==============================] - 47s 1ms/sample - loss: 0.5405 - accuracy: 0.8109 - val_loss: 0.7630 - val_accuracy: 0.7289\n",
      "Epoch 4/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.4231 - accuracy: 0.8527\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.76300\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.4228 - accuracy: 0.8529 - val_loss: 0.7780 - val_accuracy: 0.7372\n",
      "Epoch 5/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.8862\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.76300\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.3292 - accuracy: 0.8862 - val_loss: 0.8270 - val_accuracy: 0.7377\n",
      "Epoch 6/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.2731 - accuracy: 0.9052\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.76300\n",
      "43903/43903 [==============================] - 47s 1ms/sample - loss: 0.2728 - accuracy: 0.9053 - val_loss: 0.8992 - val_accuracy: 0.7351\n",
      "... Validating on Fold 2 ...\n",
      "Train on 43903 samples, validate on 10976 samples\n",
      "Epoch 1/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 1.2164 - accuracy: 0.4977\n",
      "Epoch 00001: val_loss improved from inf to 0.91977, saving model to ./storage/writer_trainfiles2/kfold2/epoch_001_val_0.920.h5\n",
      "43903/43903 [==============================] - 55s 1ms/sample - loss: 1.2156 - accuracy: 0.4982 - val_loss: 0.9198 - val_accuracy: 0.6481\n",
      "Epoch 2/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.7638 - accuracy: 0.7225\n",
      "Epoch 00002: val_loss improved from 0.91977 to 0.75502, saving model to ./storage/writer_trainfiles2/kfold2/epoch_002_val_0.755.h5\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.7638 - accuracy: 0.7224 - val_loss: 0.7550 - val_accuracy: 0.7249\n",
      "Epoch 3/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.5323 - accuracy: 0.8161\n",
      "Epoch 00003: val_loss improved from 0.75502 to 0.73346, saving model to ./storage/writer_trainfiles2/kfold2/epoch_003_val_0.733.h5\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.5323 - accuracy: 0.8161 - val_loss: 0.7335 - val_accuracy: 0.7361\n",
      "Epoch 4/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.4051 - accuracy: 0.8610\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.73346\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.4051 - accuracy: 0.8610 - val_loss: 0.7598 - val_accuracy: 0.7389\n",
      "Epoch 5/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8906\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.73346\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.3163 - accuracy: 0.8906 - val_loss: 0.8397 - val_accuracy: 0.7306\n",
      "Epoch 6/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.2695 - accuracy: 0.9073\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.73346\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.2693 - accuracy: 0.9073 - val_loss: 0.9038 - val_accuracy: 0.7321\n",
      "... Validating on Fold 3 ...\n",
      "Train on 43903 samples, validate on 10976 samples\n",
      "Epoch 1/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 1.1793 - accuracy: 0.5329\n",
      "Epoch 00001: val_loss improved from inf to 0.89601, saving model to ./storage/writer_trainfiles2/kfold3/epoch_001_val_0.896.h5\n",
      "43903/43903 [==============================] - 56s 1ms/sample - loss: 1.1789 - accuracy: 0.5332 - val_loss: 0.8960 - val_accuracy: 0.6685\n",
      "Epoch 2/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.7104 - accuracy: 0.7478\n",
      "Epoch 00002: val_loss improved from 0.89601 to 0.73925, saving model to ./storage/writer_trainfiles2/kfold3/epoch_002_val_0.739.h5\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.7100 - accuracy: 0.7479 - val_loss: 0.7392 - val_accuracy: 0.7312\n",
      "Epoch 3/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.5122 - accuracy: 0.8212\n",
      "Epoch 00003: val_loss improved from 0.73925 to 0.73728, saving model to ./storage/writer_trainfiles2/kfold3/epoch_003_val_0.737.h5\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.5126 - accuracy: 0.8212 - val_loss: 0.7373 - val_accuracy: 0.7325\n",
      "Epoch 4/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.8618\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.73728\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.4014 - accuracy: 0.8616 - val_loss: 0.8009 - val_accuracy: 0.7334\n",
      "Epoch 5/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3179 - accuracy: 0.8894\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.73728\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.3180 - accuracy: 0.8893 - val_loss: 0.8317 - val_accuracy: 0.7373\n",
      "Epoch 6/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.2679 - accuracy: 0.9058\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.73728\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.2683 - accuracy: 0.9057 - val_loss: 0.9048 - val_accuracy: 0.7358\n",
      "... Validating on Fold 4 ...\n",
      "Train on 43903 samples, validate on 10976 samples\n",
      "Epoch 1/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 1.1842 - accuracy: 0.5133\n",
      "Epoch 00001: val_loss improved from inf to 0.92419, saving model to ./storage/writer_trainfiles2/kfold4/epoch_001_val_0.924.h5\n",
      "43903/43903 [==============================] - 55s 1ms/sample - loss: 1.1835 - accuracy: 0.5135 - val_loss: 0.9242 - val_accuracy: 0.6446\n",
      "Epoch 2/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.7341 - accuracy: 0.7357\n",
      "Epoch 00002: val_loss improved from 0.92419 to 0.78948, saving model to ./storage/writer_trainfiles2/kfold4/epoch_002_val_0.789.h5\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.7336 - accuracy: 0.7360 - val_loss: 0.7895 - val_accuracy: 0.7155\n",
      "Epoch 3/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.5327 - accuracy: 0.8146\n",
      "Epoch 00003: val_loss improved from 0.78948 to 0.77015, saving model to ./storage/writer_trainfiles2/kfold4/epoch_003_val_0.770.h5\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.5330 - accuracy: 0.8145 - val_loss: 0.7702 - val_accuracy: 0.7268\n",
      "Epoch 4/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8559\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.77015\n",
      "43903/43903 [==============================] - 47s 1ms/sample - loss: 0.4174 - accuracy: 0.8559 - val_loss: 0.7961 - val_accuracy: 0.7300\n",
      "Epoch 5/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.8861\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.77015\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.3291 - accuracy: 0.8861 - val_loss: 0.8625 - val_accuracy: 0.7301\n",
      "Epoch 6/250\n",
      "43776/43903 [============================>.] - ETA: 0s - loss: 0.2794 - accuracy: 0.9019\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.77015\n",
      "43903/43903 [==============================] - 48s 1ms/sample - loss: 0.2793 - accuracy: 0.9020 - val_loss: 0.9136 - val_accuracy: 0.7207\n",
      "... Validating on Fold 5 ...\n",
      "Train on 43904 samples, validate on 10975 samples\n",
      "Epoch 1/250\n",
      "43776/43904 [============================>.] - ETA: 0s - loss: 1.1954 - accuracy: 0.5156\n",
      "Epoch 00001: val_loss improved from inf to 0.96765, saving model to ./storage/writer_trainfiles2/kfold5/epoch_001_val_0.968.h5\n",
      "43904/43904 [==============================] - 55s 1ms/sample - loss: 1.1951 - accuracy: 0.5159 - val_loss: 0.9676 - val_accuracy: 0.6397\n",
      "Epoch 2/250\n",
      "43776/43904 [============================>.] - ETA: 0s - loss: 0.7491 - accuracy: 0.7339\n",
      "Epoch 00002: val_loss improved from 0.96765 to 0.77729, saving model to ./storage/writer_trainfiles2/kfold5/epoch_002_val_0.777.h5\n",
      "43904/43904 [==============================] - 48s 1ms/sample - loss: 0.7492 - accuracy: 0.7339 - val_loss: 0.7773 - val_accuracy: 0.7193\n",
      "Epoch 3/250\n",
      "43776/43904 [============================>.] - ETA: 0s - loss: 0.5426 - accuracy: 0.8104\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.77729\n",
      "43904/43904 [==============================] - 48s 1ms/sample - loss: 0.5428 - accuracy: 0.8103 - val_loss: 0.7820 - val_accuracy: 0.7287\n",
      "Epoch 4/250\n",
      "43776/43904 [============================>.] - ETA: 0s - loss: 0.4088 - accuracy: 0.8583\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.77729\n",
      "43904/43904 [==============================] - 47s 1ms/sample - loss: 0.4085 - accuracy: 0.8584 - val_loss: 0.8459 - val_accuracy: 0.7285\n",
      "Epoch 5/250\n",
      "43776/43904 [============================>.] - ETA: 0s - loss: 0.3307 - accuracy: 0.8847\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.77729\n",
      "43904/43904 [==============================] - 48s 1ms/sample - loss: 0.3304 - accuracy: 0.8848 - val_loss: 0.9008 - val_accuracy: 0.7338\n"
     ]
    }
   ],
   "source": [
    "# conduct KFold Ensemble  \n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 777) \n",
    "for idx, (train_idx,val_idx) in enumerate(kfold.split(train_padded, y_train)):\n",
    "    \n",
    "    print(\"... Validating on Fold {} ...\".format(idx+1))\n",
    "    \n",
    "    # split data into train and validation sets \n",
    "    cur_x_train, cur_x_val = train_padded[train_idx], train_padded[val_idx] \n",
    "    cur_y_train, cur_y_val = y_train[train_idx], y_train[val_idx] \n",
    "    \n",
    "    # build model, define callbacks and train  \n",
    "    model_path = './storage/writer_trainfiles2/kfold' + str(idx+1) + '/epoch_{epoch:03d}_val_{val_loss:.3f}.h5'\n",
    "    model = bidirectional_lstm() \n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 1, verbose = 1, factor = 0.8)\n",
    "    checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 3) \n",
    "    history = model.fit(cur_x_train,\n",
    "                        cur_y_train,\n",
    "                        validation_data = (cur_x_val,cur_y_val),\n",
    "                        shuffle = True,\n",
    "                        batch_size = 256, \n",
    "                        epochs = 250,\n",
    "                        verbose = 1,\n",
    "                        callbacks = [learning_rate_reduction, checkpoint, early_stopping]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('./storage/writer_trainfiles2/kfold1/epoch_004_val_0.752.h5')\n",
    "model2 = load_model('./storage/writer_trainfiles2/kfold1/epoch_003_val_0.763.h5')\n",
    "model3 = load_model('./storage/writer_trainfiles2/kfold2/epoch_003_val_0.733.h5')\n",
    "model4 = load_model('./storage/writer_trainfiles2/kfold3/epoch_003_val_0.737.h5')\n",
    "model5 = load_model('./storage/writer_trainfiles2/kfold4/epoch_003_val_0.770.h5')\n",
    "model6 = load_model('./storage/writer_trainfiles2/kfold5/epoch_002_val_0.777.h5')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred1 = model1.predict_proba(test_padded)\n",
    "pred2 = model2.predict_proba(test_padded) \n",
    "pred3 = model3.predict_proba(test_padded) \n",
    "pred4 = model4.predict_proba(test_padded) \n",
    "pred5 = model5.predict_proba(test_padded) \n",
    "pred6 = model6.predict_proba(test_padded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_avg = (pred2 + pred3 + pred4 + pred5 + pred6)/5.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.530573</td>\n",
       "      <td>0.205267</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>0.073928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.170795</td>\n",
       "      <td>0.509856</td>\n",
       "      <td>0.077767</td>\n",
       "      <td>0.062893</td>\n",
       "      <td>0.178689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.747569</td>\n",
       "      <td>0.216619</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>0.006922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.968447</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.021397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.408807</td>\n",
       "      <td>0.347093</td>\n",
       "      <td>0.024943</td>\n",
       "      <td>0.114185</td>\n",
       "      <td>0.104972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         0         1         2         3         4\n",
       "0      0  0.068966  0.530573  0.205267  0.121267  0.073928\n",
       "1      1  0.170795  0.509856  0.077767  0.062893  0.178689\n",
       "2      2  0.747569  0.216619  0.018814  0.010077  0.006922\n",
       "3      3  0.004944  0.001761  0.968447  0.003451  0.021397\n",
       "4      4  0.408807  0.347093  0.024943  0.114185  0.104972"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss[['0','1','2','3','4']] = pred_avg \n",
    "ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('./storage/bidirectional_lstm.csv', index = False, encoding = 'utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
